% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{multirow}
\usepackage{booktabs}

% Fixup hyphenation
\hyphenation{cor-res-pon-din}
\hyphenation{cor-re-lat-tion}
\hyphenation{compa-rison}
\hyphenation{compa-risons}
\hyphenation{ini-tial-iza-tion}
\hyphenation{in-clud-ing}

\newcommand{\etal}{\textit{et al.}}


\begin{document}
%
\title{Embedding to Reference t-SNE Space Adresses Batch-Effects in Single-Cell Classification}
%
\titlerunning{t-SNE Embedding and Batch Effects}
%
\author{Pavlin G. Poli\v{c}ar\inst{1} \and
Martin Stra\v{z}ar\inst{1} \and
Bla\v{z} Zupan\inst{1,2}}
%
\authorrunning{P. G. Poli\v{c}ar, M. Stra\v{z}ar, and B. Zupan}
%
\institute{University of Ljubljana, SI-1000 Ljubljana, Slovenia 
\email{\{pavlin.policar,martin.strazar,blaz.zupan\}@fri.uni-lj.si}\\[2pt]
\and
Baylor College of Medicine, Houston, TX 77030, USA
}

\maketitle

\begin{abstract}
Dimensionality reduction techniques, such as t-SNE, can construct informative visualizations of high-dimensional data. When working with multiple data sets, the straightforward application of these methods often fails, and instead of revealing underlying classes, the resulting visualizations expose domain-specific clusters. To circumvent these batch effects, we here propose an embedding procedure that takes a t-SNE visualization constructed on reference data set and uses it as a scaffold for new embeddings. New, secondary data is embedded one data-instance at the time, thus disregarding any interactions between data items in the secondary data and implicitly mitigating batch effects. We demonstrate the utility of this procedure on analysis of six recently published single-cell gene expression data sets containing up to tens of thousands of cells and thousands of genes. In these data sets, the batch effects were particularly strong as the data comes from different institutions and was obtained using different experimental protocols. The visualizations constructed by the proposed approach are absent of batch effects, and the cells from secondary data sets correctly co-cluster with cells from the primary data sharing the same cell type.

\keywords{Batch effects \and Embedding \and t-SNE \and Visualisation \and Single-Cell Transcriptomics \and Data Integration \and Domain Adaptation.}
\end{abstract}


\section{Introduction}

Two-dimensional embeddings and their visualizations may assist in the analysis and interpretation of multi-dimensional data. Intuitively, two data instances should be co-located in the resulting visualization if their multi-dimensional profiles are similar. For this task, non-linear local embeddings such as t-distributed stochastic neighbor embedding (t-SNE)~\cite{tsne} or uniform manifold approximation and projection~\cite{umap} have recently complemented traditional data transformation and embedding approaches such as principal component analysis (PCA) and multi-dimensional scaling~\cite{distill,umap_single_cell}. While useful for visualizing the data from a single coherent source, these methods may encounter problems if the data comes from different sources. Here, if executing the dimensionality reduction procedures on a merged data, the resulting visualizations would typically depict source-specific clusters instead of reveal clusters that are semantically enriched, say, with same-type of data instances across different sources. This source-specific confounding is often referred to as {\em domain shift}~\cite{domain_shift}, {\em covariate shift}~\cite{covariate_shift} or {\em dataset shift}~\cite{dataset_shift}. In the bioinformatics literature, the domain-specific differences are more commonly referred to as {\em batch effects}~\cite{cca,mnn,seurat}.

Massive, multi-variate biological data sets typical suffer from source-specific biases. Consider an example from single-cell genomics, a domain we will focus on in this manuscript and that was --- besides current scientific challenges --- selected also due to availability and the abundance of recently published data. Single-cell data sets may include thousands of cells whose profiles consist of tens of thousands of cell-specific gene expressions. Single cell studies typically start with the analysis of cell types, where we expect that cells of the same type would cluster in two-dimensional data visualisation~\cite{seurat}. For instance, Fig.~\ref{fig:batch_effect}.a shows t-SNE embedded data from mouse brain cells originating from visual cortex~\cite{hrvatin2018} and hypothalamus~\cite{chen2017}. The figure reveals distinct clusters but also separates the data from the two brain regions. These two regions share the same cell types and --- contrary to the depiction in Fig.~\ref{fig:batch_effect}.a --- we would expect the data points from the two studies to overlap. Batch effects similarly prohibit the utility of t-SNE in the exploration of pancreas cells in Fig.~\ref{fig:batch_effect}.b, which renders the data from human cell atlas~\cite{baron2016} and similarly-typed cells from diabetic patients~\cite{xin2016}. Just like with data from brain cells, pancreas cells cluster first according to the data source, again resulting in an uninformative visualization that primarily exposes the batch effect.

\begin{figure}[htbp]
\includegraphics[width=\textwidth]{figures/batch_effect.pdf}
\caption{Batch effects are a driving factor of variation between the data sets. Depicted is a t-SNE visualisation of two pairs of data sets. In each pair, the data sets share cell types, so it would be expected that the cells from the reference data (blue) would mix with the cells in a secondary data sets (orange). Instead, t-SNE visualisation clusters data according to the data source.} \label{fig:batch_effect}
\end{figure}

Current solutions to embedding the data from various data sources address the batch effect problems up-front. The data is typically pre-processed and transformed, possibly to the latent space, so that the batch effects are removed. The recently proposed procedures for batch-effect removal include canonical correlation analysis~\cite{cca} and mutual nearest-neighbors~\cite{mnn,seurat}. In these works, a proof for the success of batch effect removal is a good mixing of cells from different sources in a t-SNE visualization. Elimination of batch effects may require aggressive data preprocessing which may blur the boundaries between cell types. Another problem is also the inclusion of any new data, for which the entire data analysis pipeline has to be rerun, usually resulting in different layout and clusters that have little resemblance to original visualization and thus require reinterpretation.

We propose a direct solution of rendering t-SNE visualizations that addresses batch effects. Our approach considers one of the data sets as a {\em reference} and aims to visually classify the cells in the other, {\em secondary data set}. We construct the t-SNE embedding using the reference data set, and then use it as a scaffold for the embedding of data points from the secondary data. Embedding is performed one data point at a time. Independence of each new embedding of data instances from secondary data set causes clustering landscape to only depend on a reference scaffold, thus removing data source-driven variation. In other words, when including new data, the scaffold inferred from the reference data set is kept unchanged and defines the ``gravitational field'' to independently embedded each new data item. For example, in Fig.~\ref{fig:transform_brain}, the cells from visual cortex define the scaffold (Fig.~\ref{fig:transform_brain}.a) into which we embed the cells from hypothalamus (Fig.~\ref{fig:transform_brain}.b). Unlike in their joint t-SNE visualization (Fig.~\ref{fig:batch_effect}.a), the hypothalamus cells are dispersed across entire embedding space and their cell type correctly matches the prevailing type in reference clusters.

At the core of the proposed solution is a mapping that implements embedding of new data to existing t-SNE visualization. In the following, we introduce t-SNE, then describe its recently proposed multi-scale extension, and outline our algorithm that embeds new data into a previously trained scaffold. While a utility of such an algorithm was already hinted to in recent publication~\cite{art_of_using_tsne}, we here provide its practical and theoretically-grounded implementation. Considering the abundance of recent publications in batch effect removal, we present surprising evidence that a computationally more direct and elegant embedding procedure solves the batch effects problem when constructing interpretable visualizations from different data sources.


\section{Methods}

We describe an end-to-end pipeline that uses t-SNE embeddings as a scaffold for new data samples, and enables visualisation of data from different sources while mitigating batch effects. Our proposed approach starts with t\nobreakdash -SNE to embedd a reference data set, with the aim of constructing a two-dimensional visualisation to facilitate interpretation and cluster classification. We regard the resulting two-dimensional embedding as a scaffold to which we add samples from the new, secondary data source. Each new sample is then placed into the reference embedding and optimized independently using the t\nobreakdash -SNE loss function. Independent embedding of each data item from a secondary data set disregards any interactions present in that data, and prevents forming clusters that would be data-souce specific. Below, we start with a summary of t-SNE and its extensions (Sec.~\ref{sec:tsne}, emphasizing the notation and relevant parts upon which we base our secondary data embedding approach (Sec.~\ref{sec:transfer}).


\subsection{Data embedding by t-SNE and its extensions\label{sec:tsne}}

The embedding by t-SNE is local and non-linear, tailored to visualisation of high dimensional data sets. Given a multi-dimensional data set $\mathbf{X} = \left \{ \mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_N \right \} \in \mathbb{R}^D$ where $N$ is the number of samples in the reference data set, t-SNE aims to find a low dimensional embedding $\mathbf{Y} = \left \{ \mathbf{y}_1, \mathbf{y}_2, \dots, \mathbf{y}_N \right \} \in \mathbb{R}^d$ where $d \ll D$, such that if points $\mathbf{x}_i$ and $\mathbf{x}_j$ are close in the multi-dimensional space, their corresponding embeddings $\mathbf{y}_i$ and $\mathbf{y}_j$ are also close. Since t-SNE is primarily used as a visualization tool, $d$ is typically set to two. The similarity between two data points in t-SNE is defined as:

\begin{equation}
p_{j \mid i} = \frac{\exp \left ( -\frac{1}{2} \mathcal{D}(\mathbf{x}_i, \mathbf{x}_j ) / \sigma_i^2 \right )}
{\sum_{k \neq \i } \exp \left ( -\frac{1}{2} \mathcal{D}(\mathbf{x}_i, \mathbf{x}_k ) / \sigma_i^2 \right )}, \quad p_{i \mid i} = 0
\label{eq:gaussian_kernel}
\end{equation}

\noindent where $\mathcal{D}$ is some distance measure. This is then symmetrized to

\begin{equation}
p_{ij} = \frac{p_{j \mid i} + p_{i \mid j}}{2N}.
\label{eq:symmetrize}
\end{equation}

The bandwidth of each Gaussian kernel $\sigma_i$ is selected such that the perplexity of the distribution matches a user-specified parameter value

\begin{equation}
\text{Perplexity} = 2^{H(P_i)}
\end{equation}

\noindent where $H(P_i)$ is the Shannon entropy of $P_i$,

\begin{equation}
H(P_i) = -\sum_i p_{j \mid i} \log_2 (p_{j \mid i}).
\end{equation}

\noindent Different bandwidths $\sigma_i$ enable t-SNE to adapt to the varying density of the data in the multi-dimensional space.

The similarity between points $\mathbf{y}_i$ and $\mathbf{y}_j$ in the embedding space are defined using the $t$-distribution with one degree of freedom

\begin{equation}
q_{ij} = \frac{\left ( 1 + || \mathbf{y}_i - \mathbf{y}_j ||^2 \right )^{-1}}
{\sum_{k \neq l}\left ( 1 + || \mathbf{y}_k - \mathbf{y}_l ||^2 \right )^{-1}},
\quad q_{ii} = 0.
\label{eq:cauchy_kernel}
\end{equation}

The data transformation by t-SNE finds an embedding $\mathbf{Y}$ that minimizes the Kullback-Leibler (KL) divergence between $\mathbf{P}$ and $\mathbf{Q}$,

\begin{equation}
C = \text{KL}(\mathbf{P} \mid \mid \mathbf{Q}) = \sum_{ij} p_{ij} \log \frac{p_{ij}}{q_{ij}}.
\label{eq:kl_divergence}
\end{equation}

The time complexity needed to evaluate the similarities in Eq.~\ref{eq:cauchy_kernel} is $\mathcal{O}(N^2)$, making the applications of the algorithm impractical for any reasonably-sized data. To address larger data sets, we instead adapt a recent approach for low-rank approximation of similarity matrix based on polynomial interpolation that reduces the time complexity of t-SNE to $\mathcal{O}(N)$. This approximation enables the visualization of massive data sets possibly containing millions of data points~\cite{fi_tsne}.

The t-SNE embeddings substantially depend on the value of the perplexity parameter. The perplexity parameter can be interpreted as the number of neighbors for which the distances in the embedding space are preserved. Small values of perplexity result in tightly-packed clusters of points and lead to ignoring the long-range interactions between clusters. Larger values may result in a more globally consistent visualisations, preserving distances on a large scale and organizing clusters in a more meaningful way. Larger values of perplexity may often result in merging multiple small clusters, thus obscuring local aspects of the data~\cite{art_of_using_tsne}.

A trade-off between the local organization and global consistency may be achieved by replacing the Gaussian kernels in Eq.~\ref{eq:gaussian_kernel} with a mixture of Gaussians of varying bandwidths~\cite{multiscale_tsne}. Multi-scale kernels are defined as

\begin{equation}
p_{j \mid i} \propto \frac{1}{L} \sum_{l=1}^{L} \exp \left ( - \frac{1}{2} \mathcal{D}(\mathbf{x}_i, \mathbf{x}_j ) / \sigma_{i,l}^2 \right ), \quad p_{i \mid i} = 0
\label{eq:multiscale}
\end{equation}

\noindent where $L$ is the number of mixture components. The bandwidths $\sigma_{i,l}$ are selected in the same manner as in Eq.~\ref{eq:gaussian_kernel}, but with a different value of perplexity for each $l$. In our experiments, we used a mixture of two Gaussian kernels with perplexity values of 50 and 500. We note that a similar formulation of multi-scale kernels was proposed in~\cite{art_of_using_tsne}, and we found the resulting embeddings are visually very similar to those obtained with the approach described above (for brevity, data not shown).


\subsection{Adding new data points to reference embedding\label{sec:transfer}}

An algorithm to embedd new data point to a reference embedding consists of estimating similarities between the point and the reference data and optimizing the position of the data point in the embedding space. Unlike parametric models such as principal component analysis or autoencoders, t-SNE does not define an explicit mapping to embedding space, and embeddings need to be found through loss function optimization.

For each new data point, we first estimate its distance to each of the data instances in the reference data set. We initialize the position of the data point in embedding space to the median embedding position of its $k$ nearest neighbors from the reference data set. While we found the algorithm to be robust to choices of $k$, we use $k=10$ in our experiments.

We adapt the standard t-SNE formulation from Eqs.~\ref{eq:gaussian_kernel}~and~\ref{eq:cauchy_kernel} with

\begin{align}
p_{j \mid i} &= \frac{\exp \left ( -\frac{1}{2} \mathcal{D}(\mathbf{x}_i, \mathbf{v}_j) / \sigma_i^2 \right )}{\sum_{i} \exp \left ( -\frac{1}{2} d(\mathbf{x}_i, \mathbf{v}_j) / \sigma_i^2 \right )}, \\
q_{j \mid i} &= \frac{\left ( 1 + || \mathbf{y}_i - \mathbf{w}_j ||^2 \right )^{-1}}{\sum_{i}\left ( 1 + || \mathbf{y}_i - \mathbf{w}_j ||^2 \right )^{-1}},
\end{align}

\noindent where $\mathbf{V} = \left \{ \mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_M \right \} \in \mathbb{R}^D$ where $M$ is the number of samples in the new data set and $\mathbf{W} = \left \{ \mathbf{w}_1, \mathbf{w}_2, \dots, \mathbf{w}_M \right \} \in \mathbb{R}^d$. Additionally, we omit the symmetrization step in Eq.~\ref{eq:symmetrize}. This enables new points to be inserted into the embedding independently of one another. The gradients of $\mathbf{w}_j$ with respect to the loss (Eq.~\ref{eq:kl_divergence}) are:

\begin{equation}
\frac{\partial C}{\partial \mathbf{w}_j} = 2 \sum_i \left ( p_{j \mid i} - q_{j \mid i} \right ) \left ( \mathbf{y}_i - \mathbf{w}_j \right ) \left ( 1 + || \mathbf{y}_i - \mathbf{w}_j || ^2 \right )^{-1}
\label{eq:gradient}
\end{equation}

In the optimization step, we refine point positions using batch gradient descent. We use an adaptive learning rate scheme with momentum proposed by Jacobs to speed up the convergence~\cite{momentum}. We run gradient descent with momentum $\alpha$ set to $0.8$ for $250$ iterations, where the optimization converged for all cases in our experiments.  The time complexity needed to evaluate the gradients in Eq.~\ref{eq:gradient} is $\mathcal{O}(N \cdot M)$, however, by adapting the same polynomial interpolation based approximation, this is reduced to $\mathcal{O}(N)$.

Special care must be taken to reduce the learning rate $\eta$ as the default value in most implementations ($\eta = 200$) may cause points to ``shoot off'' from the reference embedding. This phenomenon is caused due to the embedding to already defined t-SNE space, where the distances between data points and corresponding gradients of the optimization function may be quite large. When running s standard t-SNE, points are initialized and scaled to have variance 0.0001. The resulting gradients tend to be very small during the initial phase, resulting in stable convergence. When embedding new samples, the range of the embedding is much larger, resulting in much larger gradients, and the default learning rate causes points to move very far from the reference embedding. In our experiments, we found that lowering the learning rate to $\eta \sim 0.1$ produces stable solutions. This is especially important when using the interpolation-based approximation. Our approach places a grid of interpolation points over the embedding space, where the number of grid points is determined by the embedding range. Clearly, if even one point ``shoots off'' far from the embedding, the number of required grid points escalates, substantially increasing the runtime. The reduced learning rate suppresses this issue, and does not slow the convergence because of an adaptive learning rate scheme and running the optimization for a sufficient number of steps. 

\section{Experiments and Discussion}

We apply the proposed transfer learning approach to t-SNE visualizations of single-cell data. In single-cell data sets, the data includes the variety of cells from specific tissue and characterizes the cells through the expression of its genes. In experiments, we considered several recently published data sets where cells were annotated with the cell type. Our aim was to construct t-SNE visualizations where similarly-typed cells would cluster together, despite the differences between data sources. Below, we enlist the data sets, describe single-cell specific data preprocessing procedures, and display the resulting data visualizations. Finally, we discuss the success of the proposed approach in alleviating the batch effects.


\subsection{Data}

We use three pairs of reference and secondary single-cell data sets originating from different organisms and tissues. The data in each pair were chosen so that the majority of cell types from the secondary data set were included in the reference set (Table~\ref{tab:datasets}).

\begin{table}[ht]
\begin{center}
\setlength\tabcolsep{4pt}
\begin{tabular}{l c c r c c}
\toprule
Study & Organism/Tissue & Protocol & Cells & Cell Types & Sparsity (\%) \\
\midrule
Hrvatin \etal & \multirow{2}{*}{mouse brain} & inDrop & 48,266 & 9 & 94 \\
Chen \etal & & Drop-seq & 14,437 & 6 & 93 \\[5pt]
Baron \etal & \multirow{2}{*}{human pancreas} & inDrop & 8,569 & 9 & 91 \\
Xin \etal & & SMARTer & 1,492 & 4 & 86 \\[5pt]
Macosko \etal & \multirow{2}{*}{mouse retina} & Drop-seq & 44,808 & 12 & 97 \\
Shekhar \etal & & Drop-seq & 27,499 & 5 & 96 \\
\bottomrule
\end{tabular}
\end{center}
\caption{Data sets used in our experiments. In each pair, the first data set (Hrvatin \etal, Baron \etal, and Macoscko \etal) was used as a reference. In all cases, we relied on quality control and annotations from the original studies. To facilitate comparisons, the cell annotations were harmonized using the cell type annotations from cell ontology~\cite{cell_ontology}. Notice that different RNA sequencing protocols were used to estimate gene expressions. We here report the number of cell types from each data set retained after preprocessing. Single-cell data is sparse, typically containing less than 10\% expressed genes per cell.}
\label{tab:datasets}
\end{table}


The cells in the data sets were captured from the following three tissues:
\begin{description}
\item[Mouse brain.] The data set from Hrvatin \etal~\cite{hrvatin2018} contains cells from the visual cortex exploring transcriptional changes after exposure to light. This was used as a reference for the data from Chen \etal~\cite{chen2017}, containing various cells from the mouse hypothalamus and their reaction to food deprivation. From the secondary data, we removed cells with no corresponding types in the reference, namely ependymal cells, epithelial cells, tanycytes, and unlabelled cells.

\item[Human pancreas.] The data set from Baron \etal~\cite{baron2016} was created as an atlas of pancreatic cell types. We used this set as a reference for data from Xin \etal~\cite{xin2016}, who examined transcriptional differences between healthy and type 2 diabetic patients.

\item[Mouse retina.] The data set from Macosko \etal~\cite{macosko2015} was created as an atlas of mouse retinal cell types. We used this as a reference for the data from Shekhar \etal~\cite{shekhar2016}, who built an atlas for different types of retinal bipolar cells.
\end{description}

\subsection{Single-cell data preprocessing pipeline}

Due to the specific nature of single-cell data, additional steps must be taken to properly apply t-SNE. We use a standard single-cell preprocessing pipeline, consisting of selection of 3,000 representative genes (see Sec.~\ref{sec:gene-selection}), library size normalization, log-transformation, standardization, and PCA-based representation that retains 50 principal components~\cite{seurat,scanpy}. To obtain the reference embedding, we apply multi-scale t-SNE using PCA initialization~\cite{art_of_using_tsne}. Due to high-dimensionality of the preprocessed input data we use cosine distance to estimate similarities between reference data points~\cite{Domingos2012-CACM}. When adding new data points from secondary data set to the reference embedding, we select 1,000 genes present in both data sets and use these to estimate the similarities between the secondary data item and reference data points. The similarities are estimated using cosine similarity. We note that similarity is computed using the raw count matrices. The preprocessing stages are detailed in accompanying Python notebooks (Sec.~\ref{sec:implementation}).

\subsection{Gene selection\label{sec:gene-selection}}

Single-cell data sets suffer from a high level of technical noise and low capture efficiency, resulting in sparse expression matrices~\cite{umi}. To address this problem, we use a specialized feature-selection method, which exploits the mean-dropout relationship of expression counts as recently proposed by Kobak and Berens~\cite{art_of_using_tsne}. Here, genes with higher than expected dropout rate are regarded as potential markers for cell subpopulations and are retained in the data.

Given an expression matrix $\mathbf{X} \in \mathbb{R}^{N \times G}$ where $N$ is the number of samples and $G$ is the number of genes in the data set, we compute the fraction of cells where a gene $g$ was not expressed

\begin{equation}
d_g = \frac{1}{N} \sum_i I \left ( X_{ig} = 0\right )
\end{equation}

\noindent The mean $\log_2$ expression of the genes is computed from all the cells where gene was expressed:

\begin{equation}
m_g = \left \langle \, \log_2 X_{ig} \mid X_{ig} > t \, \right \rangle.
\end{equation}

All genes expressed in less than ten cells are discarded. In order to select a specific number of $\hat{G}$ genes, we use a binary search to find a value $b$ such that

\begin{equation}
\sum_g I \left (d_g > \exp \left [ -(m_g - b) \right ] + 0.02 \right ) = \hat{G}.
\end{equation}

\noindent In our experiments we use $t=0$ and $a=1$.


\subsection{Results and Discussion}

Figs.~\ref{fig:transform_brain}, \ref{fig:transform_pancreas}, and \ref{fig:transform_retina} show the embedding of the reference data sets and their corresponding embeddings of the secondary data sets. In all the figures, the cells from the secondary data sets were positioned in the cluster of same-typed reference cells, providing strong evidence of the success of the proposed approach. There are some deviations to these observations; for instance, in Fig.~\ref{fig:transform_brain} several oligodendrocytes precursor cells (OPC) were mapped to oligodendrocytes. This may be due to differences in annotation criteria by different authors, or due to inherent similarities of these types of cells. Examples of such erroneous placements can be found in other figures as well, but they are not common and constitute less then 5\% of the cells (less than \%1 for pancreas, \%2 for retina and 5\% for brain secondary data sets).

Notice that we could simulate the split between reference and secondary data sets by cross-validation using one data set only, as this type of experiments would not incorporate batch effects. We want to remind the reader that handling batch effects were central to our endeavor and that disregard of this effect could lead to data visualizations strikingly different from ours. For example, compare the visualisations from Fig~\ref{fig:batch_effect}.a and Fig.~\ref{fig:transform_brain}.b, or Figs.~\ref{fig:batch_effect}.b and \ref{fig:transform_pancreas}.b.

\begin{figure}[htbp]
\includegraphics[width=\textwidth]{figures/transform_brain.pdf}
\caption{Two-dimensional embedding of a reference brain cells (a) and corresponding mapping of secondary data from hypothalamus cells (b). Notice that the majority of hypothalamus cells were mapped to their corresponding reference cluster. For instance, the astrocyte cells marked with red on the right were mapped to an oval cluster of same-typed cells denoted with the same color in the visualization on the left.} \label{fig:transform_brain}
\end{figure}


% FIGURE: 2. t-SNE transform use-case
\begin{figure}[htbp]
\includegraphics[width=\textwidth]{figures/transform_pancreas.pdf}
\caption{Embedding of pancreas cells from Baron \etal~\cite{baron2016} and cells from the same tissue from Xin \etal.~\cite{xin2016}. Just like in Fig~\ref{fig:transform_brain} vast majority of the cells from the secondary data set were correctly mapped to the same-typed cluster of reference cells.}\label{fig:transform_pancreas}
\end{figure}


\begin{figure}[htbp]
\includegraphics[width=\textwidth]{figures/transform_retina.pdf}
\caption{Embedding of a large reference retina cells from Macosco \etal~\cite{macosko2015} (a) and mapping of cells from a smaller study that focuses on bipolar cells from Shekhar \etal~\cite{shekhar2016} (b). We use colors consistent with the study by Macosko \etal. Notice that Shekhar includes cells of only four different types.} \label{fig:transform_retina}
\end{figure}

There are a few tricks that we use in our procedure for t-SNE embedding of the secondary data set that were proposed recently and enhance the original t-SNE. An important one is a multi-scale extension that besides local ordering of the data points takes care about global optimization of the cluster placement. For illustration, consider visualizations with standard and multi-scale t-SNE in Fig.~\ref{fig:multiscale}. Notice, for instance, that in multi-scale t-SNE (Fig.~\ref{fig:multiscale}.b) the clusters with neuron cells are clumped together, while their placement in standard t-SNE is arbitrary (Fig.~\ref{fig:multiscale}.a).


\begin{figure}[htbp]
\includegraphics[width=\textwidth]{figures/hrvatin_multiscale_tsne.pdf}
\caption{Comparison of embedding by stanard and multiscale t-SNE on a data set from the mouse visual cortex~\cite{hrvatin2018}. {\bf (a)} Standard t-SNE places clusters arbitrarily. {\bf (b)} Augmenting t-SNE with multi-scale similarities provides a more meaningful layout of the clusters. Neuronal types occupy one region of the space. Oligodendrocyte precursor cells (OPCs) are mainly progenitors to oligodendrocytes, but may also differentiate into neurons or astrocytes.} \label{fig:multiscale}
\end{figure}

We have also observed an important role of gene selection in crafting the reference embedding spaces. We have found that when selecting an insufficient number of genes, the resulting visualizations display fragmented clusters. When the selection is too broad and includes lowly expressed genes, the subclusters tend to overlap. These effects can all be attributed to sparseness of the data sets and may be intrinsic for single-cell data. In our studies, we found that selection of 3,000 genes yields most informative visualizations (Fig.~\ref{fig:gene_selection}).

\begin{figure}[htbp]
\includegraphics[width=\textwidth]{figures/hrvatin_embedding_tsne_genes.pdf}
\caption{Gene selection plays an important role when constructing the reference embedding. {\bf (a)} Using too few genes results in over-clustering. {\bf (b)} Using an intermediate number of genes reveals clustering mostly consistent with cell annotations. {\bf (c)} Including all the genes may lead to under-clustering of the more specialized cell types.}
\label{fig:gene_selection}
\end{figure}

In principle, our theoretically-grounded embedding of secondary data into the scaffold defined by reference embedding could be simplified with the application of the nearest neighbors-based procedure. For example, while describing a set of tricks for t-SNE, Kobak and Berens~\cite{art_of_using_tsne} proposed to position new points into a known embedding by placing them in the median position of their 10 nearest neighbors, where the neighborhood was estimated in the original data space. Notice that we use this trick as well, but only for initialization of positions of new data instances that are subject to further optimization. In Fig.~\ref{fig:optimization} we demonstrate that nearest neighboring-based positioning is insufficient and may yield clumped visualizations where the optimal positioning using the t-SNE loss function is much more dispersed and rightfully shows a more considerable variation in the secondary data. Some data points may also fell into the neighboring space between differently typed clusters, while after optimization they typically converge closer to same-typed groups.

\begin{figure}[htbp]
\includegraphics[width=\textwidth]{figures/optimization_retina.pdf}
\caption{Comparison of data placement by nearest neighbors approach from Kobak and Berens~\cite{art_of_using_tsne} and optimized placement by our algorithm. {\bf (a)} Data points are placed to the median position of their 10 nearest neighbors in the reference set. {\bf (b)} Point positions are optimized, revealing a different, more dispersed placement that better reflects the variety of cells in the secondary data set.}
\label{fig:optimization}
\end{figure}

The proposed method assumes that all cell types from the secondary data set are present in the reference. The proposed method would fail to reveal novel cell types in the secondary data set, possibly positioning them arbitrarily close to unrelated clusters. Procedures such as scmap~\cite{scmap} were recently proposed to cope with such cases and identify the cells whose type is new and not included in the reference. Our procedure does not address such cases, and for scaling-up to a wider collection of cell types relies on emerging availability of large collections of the reference data such as those managed by Human Cell Atlas initiative~\cite{hca}. 

\subsection{Implementation\label{sec:implementation}}

The procedures described in this paper are provided as Python notebooks that are, together with the data, available in an open repository~\footnote{https://github.com/biolab/tsne-embedding}. All experiments were run using openTSNE~\footnote{https://github.com/pavlin-policar/openTSNE}, our open and extensible t-SNE library for Python.

\section{Conclusion}

Almost all recent publications of single-cell studies start with a two-dimensional visualization of the data that exposes the diversity as well as different types of cells from the study. While any dimensionality reduction technique can be used to render such a visualization, different variants of t-SNE are most often used. Due to the ability to explore biological mechanisms at the cellular level, single-cell studies are increasingly widespread, and their publications in the past couple of years are abundant. One of the central tasks in single-cell studies is the classification of new cells based on finding from previous studies. Such transfer of knowledge is often difficult due to batch effects present in data from different sources. Solving the problem of the batch-effects together with prevailing means of presenting single-cell data in two-dimensional visualizations motivated the research presented in this paper. 

Our proposed approach uses t-SNE embedding as a scaffold for the positioning of new cells within the visualization, and possibly for aiding their classification. The three case studies incorporating pairs of data sets from different domains but with similar classifications demonstrate that our proposed procedure can effectively deal with batch effects to construct visualizations that correctly map a secondary data set onto a reference data set from an independent study that possibly uses different data collection protocol. While we here focused on t-SNE constructed from reference data sets, the approach could be applied to any existing two-dimensional visualization.

\subsubsection*{Acknowledgements}

This work was supported by the Slovenian Research Agency Program Grant P2-0209, and by the BioPharm.SI project supported from European Regional Development Fund and the Slovenian Ministry of Education, Science and Sport. We would also like to thank Dmitry Kobak for helpful discussions on t-SNE extensions.



\bibliographystyle{splncs04}
\bibliography{references}
\end{document}